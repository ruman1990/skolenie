# ğŸ“˜ **Modul `urllib` v Pythone**

Modul **`urllib`** je sÃºÄasÅ¥ Å¡tandardnej kniÅ¾nice Pythonu, Äo znamenÃ¡, Å¾e ho moÅ¾no pouÅ¾Ã­vaÅ¥ bez inÅ¡talÃ¡cie ÄalÅ¡Ã­ch balÃ­kov. Poskytuje nÃ¡stroje na prÃ¡cu s URL adresami, posielanie HTTP poÅ¾iadaviek, dekÃ³dovanie parametrov Äi ÄÃ­tanie robots.txt.

PozostÃ¡va zo 4 hlavnÃ½ch podmodulov:

1. `urllib.request` â€“ odosielanie HTTP poÅ¾iadaviek (GET, POST, hlaviÄky...)
2. `urllib.parse` â€“ parsovanie a skladanie URL adries
3. `urllib.error` â€“ zachytÃ¡vanie HTTP a URL chÃ½b
4. `urllib.robotparser` â€“ ÄÃ­tanie a spracovanie robots.txt

---

# ğŸ§© **1. urllib.request â€“ odosielanie HTTP poÅ¾iadaviek**

Toto je najdÃ´leÅ¾itejÅ¡ia ÄasÅ¥ `urllib`. UmoÅ¾Åˆuje:

* odoslaÅ¥ HTTP GET a POST poÅ¾iadavku,
* zÃ­skavaÅ¥ odpoveÄ zo servera,
* ÄÃ­taÅ¥ stavovÃ© kÃ³dy a hlaviÄky,
* sÅ¥ahovaÅ¥ sÃºbory,
* pracovaÅ¥ s vlastnÃ½mi hlaviÄkami.

## 1.1 ZÃ¡kladnÃ½ GET request

NajjednoduchÅ¡Ã­ spÃ´sob, ako naÄÃ­taÅ¥ strÃ¡nku:

```python
from urllib import request

response = request.urlopen("https://www.example.com")

html = response.read().decode("utf-8")
print(html)
```

Objekt `response` obsahuje stavovÃ½ kÃ³d, hlaviÄky aj telo odpovede.

### DÃ´leÅ¾itÃ© metÃ³dy:

* `response.status` â€“ HTTP status (200 = OK)
* `response.getheaders()` â€“ zoznam hlaviÄiek
* `response.read()` â€“ naÄÃ­tanie dÃ¡t

---

## 1.2 Posielanie POST Å¾iadosti

POST request sa pouÅ¾Ã­va, keÄ chceme odosielaÅ¥ dÃ¡ta.

```python
from urllib import request, parse

url = "https://httpbin.org/post"
data = {"name": "Peter", "age": 21}

encoded = parse.urlencode(data).encode("utf-8")
req = request.Request(url, data=encoded, method="POST")

resp = request.urlopen(req)
print(resp.read().decode())
```

---

## 1.3 PridÃ¡vanie vlastnÃ½ch hlaviÄiek

HlaviÄky mÃ´Å¾u byÅ¥ kÄ¾ÃºÄovÃ© napr. pri pristupovanÃ­ k API.

```python
req = request.Request(
    "https://www.example.com",
    headers={"User-Agent": "Mozilla/5.0"}
)

response = request.urlopen(req)
```

---

## 1.4 SÅ¥ahovanie sÃºborov

StaÄÃ­ jedna funkcia:

```python
from urllib import request

request.urlretrieve("https://example.com/img.png", "obrazok.png")
```

---

## ğŸ“ **Ãšlohy â€“ Kapitola 1 (urllib.request)**

1. NaÄÃ­taj obsah Ä¾ubovoÄ¾nej webovej strÃ¡nky a vypÃ­Å¡ prvÃ½ch 200 znakov.
2. NapÃ­Å¡ skript, ktorÃ½ odoÅ¡le POST poÅ¾iadavku na `https://httpbin.org/post` a odoÅ¡le meno a email.
3. Stiahni akÃ½koÄ¾vek obrÃ¡zok z internetu a uloÅ¾ ho do sÃºboru `foto.png`.
4. Odosli GET poÅ¾iadavku s vlastnou hlaviÄkou `User-Agent: TestBot/1.0`.

---

# ğŸ§© **2. urllib.parse â€“ parsovanie URL**

Modul `urllib.parse` slÃºÅ¾i na:

* rozklad URL na jednotlivÃ© Äasti (scheme, hostname, pathâ€¦),
* dekÃ³dovanie a kÃ³dovanie parametrov,
* skladanie URL adries,
* parsovanie query stringov (`?name=Jano&age=20`).

---

## 2.1 Rozklad URL na Äasti

```python
from urllib import parse

url = "https://example.com/products/item?id=15&color=red"
parsed = parse.urlparse(url)

print(parsed.scheme)   # https
print(parsed.netloc)   # example.com
print(parsed.path)     # /products/item
print(parsed.query)    # id=15&color=red
```

---

## 2.2 Parsovanie query parametrov

```python
from urllib import parse

params = parse.parse_qs("name=Peter&age=30&age=31")
print(params)
```

VÃ½stup:

```
{'name': ['Peter'], 'age': ['30', '31']}
```

---

## 2.3 Skladanie URL

```python
from urllib import parse

parts = ("https", "example.com", "/login", "", "user=admin", "")
url = parse.urlunparse(parts)

print(url)
```

---

## 2.4 Encode/Decode (kÃ³dovanie Unicode textu)

```python
from urllib import parse

encoded = parse.quote("JÃ¡n Å½elinskÃ½")
print(encoded)   # J%C3%A1n%20%C5%BDelinsk%C3%BD
print(parse.unquote(encoded))
```

---

## ğŸ“ **Ãšlohy â€“ Kapitola 2 (urllib.parse)**

1. Rozparsuj URL `https://example.com/test/page?x=10&y=20` a vypÃ­Å¡ iba query parametre.
2. Z textu `"Milan Å Å¥astnÃ½"` vytvor URL-enkÃ³dovanÃº verziu.
3. Znovu zloÅ¾ URL z ÄastÃ­:

   * scheme: `https`
   * host: `api.test.com`
   * path: `/v1/user`
   * query: `id=99`
4. PreveÄ text `"python urllib modul"` na formÃ¡t vhodnÃ½ do URL a potom spÃ¤Å¥.

---

# ğŸ§© **3. urllib.error â€“ prÃ¡ca s chybami**

Pri HTTP komunikÃ¡cii Äasto nastanÃº chyby (404, 403, 500â€¦).

`urllib.error` poskytuje dve vÃ½nimky:

* `HTTPError` â€“ server odpovie chybovÃ½m HTTP kÃ³dom
* `URLError` â€“ DNS chyba, nedostupnÃ¡ strÃ¡nka, nesprÃ¡vny formÃ¡t URLâ€¦

## PrÃ­klad:

```python
from urllib import request, error

try:
    response = request.urlopen("https://example.com/neexistuje")
except error.HTTPError as e:
    print("HTTP chyba:", e.code, e.reason)
except error.URLError as e:
    print("URL chyba:", e.reason)
```

---

## ğŸ“ **Ãšlohy â€“ Kapitola 3 (urllib.error)**

1. SkÃºs naÄÃ­taÅ¥ strÃ¡nku `https://example.com/404` a zachyÅ¥ HTTPError.
2. SkÃºs spracovaÅ¥ URL `"ht!tp://zle"` a zachyÅ¥ URLError.
3. Uprav skript tak, aby pri chybe vypÃ­sal vlastnÃº sprÃ¡vu:
   â€Nepodarilo sa naÄÃ­taÅ¥ strÃ¡nku.â€œ

---

# ğŸ§© **4. urllib.robotparser â€“ robot.txt**

Modul `robotparser` umoÅ¾Åˆuje naÄÃ­taÅ¥ a interpretovaÅ¥ pravidlÃ¡ v `robots.txt`.

## 4.1 ZÃ¡kladnÃ½ prÃ­klad

```python
from urllib import robotparser

rp = robotparser.RobotFileParser()
rp.set_url("https://www.example.com/robots.txt")
rp.read()

print(rp.can_fetch("*", "https://www.example.com/secret"))
```

Funkcia `can_fetch()` povie, Äi je povolenÃ© naÄÃ­taÅ¥ danÃº URL podÄ¾a pravidiel webu.

---

## ğŸ“ **Ãšlohy â€“ Kapitola 4 (urllib.robotparser)**

1. NaÄÃ­taj robots.txt z `https://www.python.org/robots.txt`.
2. Skontroluj, Äi agent `"*"` mÃ´Å¾e pristÃºpiÅ¥ na `/downloads/`.
3. Zisti, Äi je povolenÃ½ prÃ­stup na `/private/`.

---

# ğŸ§© **5. Zhrnutie celÃ©ho modulu `urllib`**

* `urllib.request` â€“ naÄÃ­tavanie webu, requesty, sÅ¥ahovanie.
* `urllib.parse` â€“ prÃ¡ca s URL, kÃ³dovanie, dekÃ³dovanie.
* `urllib.error` â€“ spracovanie chÃ½b.
* `urllib.robotparser` â€“ interpretÃ¡cia robots.txt.

Modul je menej pohodlnÃ½ ako modernÃ© kniÅ¾nice typu **`requests`**, ale je dÃ´leÅ¾itÃ½ pre prÃ­pady, keÄ sa musÃ­ pouÅ¾iÅ¥ Å¡tandardnÃ¡ kniÅ¾nica alebo tam, kde sa kniÅ¾nice inÅ¡talovaÅ¥ nesmÃº (napr. skÃºÅ¡ky, certifikÃ¡ty, obmedzenÃ© prostredia).

---

# ğŸ **ZÃ¡vereÄnÃ© komplexnÃ© cviÄenie**

Vytvor program, ktorÃ½:

1. NaÄÃ­ta robots.txt z Ä¾ubovoÄ¾nej strÃ¡nky.
2. Skontroluje, Äi je povolenÃ© naÄÃ­taÅ¥ strÃ¡nku `/`.
3. Ak Ã¡no:

   * vykonÃ¡ GET poÅ¾iadavku,
   * uloÅ¾Ã­ kompletnÃ½ HTML obsah do sÃºboru `stranka.html`,
   * vypÃ­Å¡e Å¡tatistiku:

     * stavovÃ½ kÃ³d,
     * poÄet znakov v HTML,
     * poÄet prijatÃ½ch hlaviÄiek.
4. Ak nie:

   * vypÃ­Å¡e sprÃ¡vu â€StrÃ¡nku nie je dovolenÃ© naÄÃ­taÅ¥ podÄ¾a robots.txtâ€œ.
